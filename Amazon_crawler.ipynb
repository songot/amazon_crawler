{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: virtualenv in /srv/conda/envs/notebook/lib/python3.7/site-packages (20.0.21)\n",
      "Requirement already satisfied: six<2,>=1.9.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from virtualenv) (1.15.0)\n",
      "Requirement already satisfied: importlib-metadata<2,>=0.12; python_version < \"3.8\" in /srv/conda/envs/notebook/lib/python3.7/site-packages (from virtualenv) (1.6.0)\n",
      "Requirement already satisfied: distlib<1,>=0.3.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from virtualenv) (0.3.0)\n",
      "Requirement already satisfied: filelock<4,>=3.0.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from virtualenv) (3.0.12)\n",
      "Requirement already satisfied: appdirs<2,>=1.4.3 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from virtualenv) (1.4.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from importlib-metadata<2,>=0.12; python_version < \"3.8\"->virtualenv) (3.1.0)\n",
      "created virtual environment CPython3.7.6.final.0-64 in 430ms\n",
      "  creator CPython3Posix(dest=/home/jovyan/project/env, clear=False, global=False)\n",
      "  seeder FromAppData(download=False, pip=latest, setuptools=latest, wheel=latest, via=copy, app_data_dir=/home/jovyan/.local/share/virtualenv/seed-app-data/v1.0.1)\n",
      "  activators BashActivator,CShellActivator,FishActivator,PowerShellActivator,PythonActivator,XonshActivator\n",
      "Requirement already satisfied: selenium in ./env/lib/python3.7/site-packages (3.141.0)\n",
      "Requirement already satisfied: urllib3 in ./env/lib/python3.7/site-packages (from selenium) (1.25.9)\n",
      "Archive:  chromedriver_linux64.zip\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘project’: File exists\n",
      "File ‘chromedriver_linux64.zip’ already there; not retrieving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "python -m pip install virtualenv\n",
    "mkdir project\n",
    "cd project\n",
    "virtualenv env\n",
    ". env/bin/activate\n",
    "python -m pip install selenium\n",
    "cd env/bin/\n",
    "wget -nc https://chromedriver.storage.googleapis.com/83.0.4103.39/chromedriver_linux64.zip\n",
    "unzip -u chromedriver_linux64.zip\n",
    "cd /home/jovyan/project\n",
    "mv /home/jovyan/test.py .\n",
    "python3 test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting virtualenv\n",
      "  Downloading virtualenv-20.0.21-py2.py3-none-any.whl (4.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.7 MB 6.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting appdirs<2,>=1.4.3\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting filelock<4,>=3.0.0\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Collecting distlib<1,>=0.3.0\n",
      "  Downloading distlib-0.3.0.zip (571 kB)\n",
      "\u001b[K     |████████████████████████████████| 571 kB 22.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six<2,>=1.9.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from virtualenv) (1.15.0)\n",
      "Requirement already satisfied: importlib-metadata<2,>=0.12; python_version < \"3.8\" in /srv/conda/envs/notebook/lib/python3.7/site-packages (from virtualenv) (1.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from importlib-metadata<2,>=0.12; python_version < \"3.8\"->virtualenv) (3.1.0)\n",
      "Building wheels for collected packages: distlib\n",
      "  Building wheel for distlib (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for distlib: filename=distlib-0.3.0-py3-none-any.whl size=340427 sha256=5b8f7f5c6fc5d69325a4202537ec2de3f0f2c914deba1d45f98e0b86ff82d6ce\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/a2/19/da/a15d4e2bedf3062c739b190d5cb5b7b2ecfbccb6b0d93c861b\n",
      "Successfully built distlib\n",
      "Installing collected packages: appdirs, filelock, distlib, virtualenv\n",
      "Successfully installed appdirs-1.4.4 distlib-0.3.0 filelock-3.0.12 virtualenv-20.0.21\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install virtualenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chromium 83.0.4103.61 Built on Ubuntu , running on Ubuntu 18.04\r\n"
     ]
    }
   ],
   "source": [
    "!chromium-browser --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromeDriver 83.0.4103.39 (ccbf011cb2d2b19b506d844400483861342c20cd-refs/branch-heads/4103@{#416})\r\n"
     ]
    }
   ],
   "source": [
    "!./project/env/bin/chromedriver --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test.py\n",
    "#from selenium import webdriver\n",
    "#driver = webdriver.Chrome()\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "driver = webdriver.Chrome('/home/jovyan/project/env/bin/chromedriver',options=chrome_options)\n",
    "resp=driver.get(\"https://www.google.nl\")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash\n",
    "mkdir scrapers/\n",
    "mkdir scrapers/amazon\n",
    "cd scrapers/amazon\n",
    "python -m pip install virtualenv\n",
    "virtualenv env\n",
    ". env/bin/activate\n",
    "python -m pip install scrapy\n",
    "python -m pip install numpy\n",
    "python -m pip install pandas\n",
    "python -m pip install requests\n",
    "python -m pip install scraperapi-sdk\n",
    "scrapy startproject amazon_scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash\n",
    "python -m pip install pandas\n",
    "python -m pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash\n",
    "wget https://chromedriver.storage.googleapis.com/84.0.4147.30/chromedriver_linux64.zip\n",
    "unzip chromedriver_linux64.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "\n",
    "# instantiate a chrome options object so you can set the size and headless preference\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--window-size=1920x1080\")\n",
    "\n",
    "# download the chrome driver from https://sites.google.com/a/chromium.org/chromedriver/downloads and put it in the\n",
    "# current directory\n",
    "chrome_driver = os.getcwd() +\"\\\\chromedriver.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go to Google and click the I'm Feeling Lucky button\n",
    "driver = webdriver.Chrome(options=chrome_options, executable_path=chrome_driver)\n",
    "driver.get(\"https://www.google.com\")\n",
    "lucky_button = driver.find_element_by_css_selector(\"[name=btnI]\")\n",
    "lucky_button.click()\n",
    "\n",
    "# capture the screen\n",
    "driver.get_screenshot_as_file(\"capture.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('store_credentials.csv', header=0, sep=';', index_col=False)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile settings.py\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Scrapy settings for amazon_scraper project\n",
    "#\n",
    "# For simplicity, this file contains only settings considered important or\n",
    "# commonly used. You can find more settings consulting the documentation:\n",
    "#\n",
    "#     https://docs.scrapy.org/en/latest/topics/settings.html\n",
    "#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
    "#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "\n",
    "BOT_NAME = 'amazon_scraper'\n",
    "\n",
    "SPIDER_MODULES = ['amazon_scraper.spiders']\n",
    "NEWSPIDER_MODULE = 'amazon_scraper.spiders'\n",
    "\n",
    "\n",
    "# Crawl responsibly by identifying yourself (and your website) on the user-agent\n",
    "USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0.3 Safari/605.1.15'\n",
    "\n",
    "# Obey robots.txt rules\n",
    "ROBOTSTXT_OBEY = False\n",
    "\n",
    "# Configure maximum concurrent requests performed by Scrapy (default: 16)\n",
    "#CONCURRENT_REQUESTS = 32\n",
    "\n",
    "# Configure a delay for requests for the same website (default: 0)\n",
    "# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n",
    "# See also autothrottle settings and docs\n",
    "#DOWNLOAD_DELAY = 3\n",
    "# The download delay setting will honor only one of:\n",
    "#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n",
    "#CONCURRENT_REQUESTS_PER_IP = 16\n",
    "\n",
    "# Disable cookies (enabled by default)\n",
    "#COOKIES_ENABLED = False\n",
    "\n",
    "# Disable Telnet Console (enabled by default)\n",
    "#TELNETCONSOLE_ENABLED = False\n",
    "\n",
    "# Override the default request headers:\n",
    "#DEFAULT_REQUEST_HEADERS = {\n",
    "#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "#   'Accept-Language': 'en',\n",
    "#}\n",
    "\n",
    "# Enable or disable spider middlewares\n",
    "# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "#SPIDER_MIDDLEWARES = {\n",
    "#    'amazon_scraper.middlewares.AmazonScraperSpiderMiddleware': 543,\n",
    "#}\n",
    "\n",
    "# Enable or disable downloader middlewares\n",
    "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
    "#DOWNLOADER_MIDDLEWARES = {\n",
    "#    'amazon_scraper.middlewares.AmazonScraperDownloaderMiddleware': 543,\n",
    "#}\n",
    "\n",
    "# Enable or disable extensions\n",
    "# See https://docs.scrapy.org/en/latest/topics/extensions.html\n",
    "#EXTENSIONS = {\n",
    "#    'scrapy.extensions.telnet.TelnetConsole': None,\n",
    "#}\n",
    "\n",
    "# Configure item pipelines\n",
    "# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "#ITEM_PIPELINES = {\n",
    "#    'amazon_scraper.pipelines.AmazonScraperPipeline': 300,\n",
    "#}\n",
    "\n",
    "# Enable and configure the AutoThrottle extension (disabled by default)\n",
    "# See https://docs.scrapy.org/en/latest/topics/autothrottle.html\n",
    "#AUTOTHROTTLE_ENABLED = True\n",
    "# The initial download delay\n",
    "#AUTOTHROTTLE_START_DELAY = 5\n",
    "# The maximum download delay to be set in case of high latencies\n",
    "#AUTOTHROTTLE_MAX_DELAY = 60\n",
    "# The average number of requests Scrapy should be sending in parallel to\n",
    "# each remote server\n",
    "#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n",
    "# Enable showing throttling stats for every response received:\n",
    "#AUTOTHROTTLE_DEBUG = False\n",
    "\n",
    "# Enable and configure HTTP caching (disabled by default)\n",
    "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n",
    "#HTTPCACHE_ENABLED = True\n",
    "#HTTPCACHE_EXPIRATION_SECS = 0\n",
    "#HTTPCACHE_DIR = 'httpcache'\n",
    "#HTTPCACHE_IGNORE_HTTP_CODES = []\n",
    "#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash\n",
    "mv settings.py scrapers/amazon/amazon_scraper/amazon_scraper/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile amazon_spider.py\n",
    "import scrapy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scrapy.http import FormRequest\n",
    "\n",
    "class AmazonSpider(scrapy.Spider):\n",
    "    name = 'amazon'\n",
    "    #start_urls = ['https://sellercentral.amazon.com']\n",
    "    data = pd.read_csv('/home/jovyan/store_credentials.csv', header=0, sep=';', index_col=False)\n",
    "    user = data.Email\n",
    "    password = data.Password\n",
    "    \n",
    "    def start_requests(self):\n",
    "        return [FormRequest(\"https://sellercentral.amazon.com/signin\", formdata={\"user\":self.user,   \n",
    "           \"pass\":self.password}, callback=self.parse)]\n",
    "    def parse(self,response):\n",
    "        print('hello')\n",
    "    \n",
    "    \"\"\"\n",
    "    def get_it(self,response):\n",
    "        check = response.css('div.active a::attr(href)').get()\n",
    "        print(check)\n",
    "        \n",
    "    def login(self, response):\n",
    "        token = response.css('form input[name*=appActionToken]::attr(value)').get()\n",
    "        openid = response.css('form input[name*=openid]::attr(value)').get()\n",
    "        prevRID = response.css('form input[name*=prevRID]::attr(value)').get()\n",
    "        workflowState = response.css('form input[name*=workflowState]::attr(value)').get()\n",
    "        print(token)\n",
    "        print(openid)\n",
    "        print(prevRID)\n",
    "        print(workflowState)\n",
    "        yield FormRequest.from_response(response, formdata = {'appActionToken':token,'email':self.user,'password':self.password},callback=self.get_it)\n",
    "\n",
    "    def parse(self, response):\n",
    "        way = response.css('a.link::attr(href)').get()\n",
    "        yield scrapy.Request(way, self.login)\n",
    "        #links =\\\n",
    "        #response.css('div.organic__path a::attr(href)').getall()\n",
    "        #self.url_data = np.append(self.url_data, np.array(links))\n",
    "        #self.url_counter+=len(links)     \n",
    "        #next_page = response.css('a.pager__item_kind_next::attr(href)').get()\n",
    "        #if (self.url_counter<2500) and (next_page is not None):\n",
    "            #yield scrapy.Request(self.client.scrapyGet(url = self.search_engine+next_page), self.parse)\n",
    "    \"\"\"\n",
    "    #def closed(self, reason):\n",
    "        #print(self.url_counter)\n",
    "        #raw = pd.DataFrame(data=self.url_data)\n",
    "        #raw.to_csv(r'/home/jovyan/raw_url.csv', index = False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash \n",
    "mv amazon_spider.py scrapers/amazon/amazon_scraper/amazon_scraper/spiders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash\n",
    "cd scrapers/amazon\n",
    ". env/bin/activate\n",
    "cd amazon_scraper/\n",
    "scrapy crawl amazon"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0.3 Safari/605.1.15'\n",
    "\n",
    "<div class=\"a-section a-spacing-base\">\n",
    "              <div class=\"a-section\">\n",
    "\n",
    "                <form name=\"signIn\" method=\"post\" novalidate=\"\" action=\"https://sellercentral.amazon.com/ap/signin\" class=\"a-spacing-none\" data-fwcim-id=\"2nWX2k0T\">\n",
    "\n",
    "\n",
    "\n",
    "                  <input type=\"hidden\" name=\"appActionToken\" value=\"Cc8UOtMhm02RMxErxN0p3GXxaiIj3D\"><input type=\"hidden\" name=\"appAction\" value=\"SIGNIN\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                  <input type=\"hidden\" name=\"openid.return_to\" value=\"ape:aHR0cHM6Ly9zZWxsZXJjZW50cmFsLmFtYXpvbi5jb20vaG9tZQ==\">\n",
    "\n",
    "                  <input type=\"hidden\" name=\"prevRID\" value=\"ape:NEUzNDAySkdXQ0NLRlI3RzMzODc=\">\n",
    "\n",
    "                  <input type=\"hidden\" name=\"workflowState\" value=\"eyJ6aXAiOiJERUYiLCJlbmMiOiJBMjU2R0NNIiwiYWxnIjoiQTI1NktXIn0.SBiUsOYBUSRV0pg4lJpqKOPcJGrvgvy55NbvBNy0f96eiSzLVbpglA.noov9zdO0_fT1SJ3.5GxbjYb6JJ-9h5jia2EzJgvPntDJUnW1bfKuKaZsI7JYo0L-A-dUTEm1h52Y_NZgzeBo2BrQzZ57CQ6e7FiJnQJ6oqQODN-rEyLQCys1x32TmBGGMrX-WZHXqQTZnCzXY_Ii5M5NVngN_5mhVH3ez6lj2kNbgaOpPD0GqYenQ5tatOFeBL1FR-o--hRvZdc4FzOKiqhZ2tMoWTy3unFMt6Pc-TW_Tt9OjdsnFOrkubmAw2Z1RtUH-5xV7OVvpWXd8inIRpTrwOg3rmR7XK7qB1uOiiL-r1bDuhfRZ_v-q7-ZBom0KMqb4jFZxOtAEanpFXt45gHEdoJ9s9CZgAQT5hjN-8DPneKIHhuc9XbklkXmFSFN3Qe9kpiFiWbIENl9BLn_dh1kaQOJHhtPkgaGPiJ-OsY03DxeAL4ksA1Gzm9bLFq2LgChoevVlesx7Ec_miGKIjTKGZDSuA60on-Xiak99kJKcZdfF-3hKGGYvsKwDhjOfkyZcfbJaCMXX6ZzrMrzX0flw44XhgU9nSPdXTBWQ0AmSnL2e0STsZPbesFwkYxF1R5pXMS2DJc3WwJocK3141kkdikZHLH9Jn1evnbxm-bPMGeRAYV5h8JmEaLa2rXcmx7Qn46nxMim24_5C2LjV3YZfsmM55zHxEFEfhyxBKynmkWhfRloW008B1343E6DSlr1ePNcxGT9NAyid0xP029rYoI8HCSJYm-Ryi4XLGL_0FuR5hztGNgK_HlddRfr7ahWjtGpWIWfmFXRwQeQaQkbrVrXgxEi9lnyAoLOLRBEuHCzS7Wd_ATA9EZm3sB-_OPr6Om8byRe.gczlufWLIIN6R44S2D9AJA\">\n",
    "\n",
    "<div class=\"myo-spa-tab active\"><a data-test-id=\"tab-/mfn/unshipped\" href=\"/orders-v3/mfn/unshipped?_encoding=UTF8&amp;page=1\">\n",
    "                      <h4><span class=\"myo-spa-highlight\">5</span> <span class=\"\">Unshipped</span></h4>\n",
    "                    </a></div>\n",
    "\n",
    " <a class=\"a-link-nav-icon\" tabindex=\"-1\" href=\"/home\">\n",
    "            <img src=\"https://images-na.ssl-images-amazon.com/images/G/01/rainier/nav/sc-unified._CB420062852_.png\" />\n",
    "\n",
    "# Obey robots.txt rules\n",
    "ROBOTSTXT_OBEY = False\n",
    "\n",
    "from scrapy.http import FormRequests\n",
    "import scrapy\n",
    "def start_requests(self):\n",
    "   return [\n",
    "      FormRequest(\"INSERT URL\", formdata={\"user\":\"user\",   \n",
    "           \"pass\":\"pass\"}, callback=self.parse)]\n",
    "def parse(self,response):\n",
    "    pass\n",
    "\n",
    "<a href=\"https://sellercentral.amazon.com/signin\" data-metric-name=\"sc:welcomePage:login\" class=\"link\">\n",
    "\n",
    "<a href=\"https://services.amazon.com/content/sell-on-amazon.htm/ref=sc_us_soa_strip?ld=SCSOAStriplogin\" data-metric-name=\"sc:welcomePage:service-ad:selling-on-amazon:US\" class=\"dropdownItem\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
